# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P3FInBzXwutXh5hBexDjLX57pvfr0RPi
"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score
from sentence_transformers import SentenceTransformer
import numpy as np

import pandas as pd

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

def dim_red(mat, p, method, labels):
    if method=='ACP':
          # Initialize PCA object
        pca = PCA(n_components=20)

        # Fit PCA on embeddings
        pca.fit(mat)

        # Apply dimensionality reduction
        df_pca = pd.DataFrame(pca.transform(mat))

        # Add label column to the new dataframe
        df_pca['label'] = labels

        # Rename columns
        df_pca.columns = ['PC'+str(i+1) for i in range(20)] + ['label']
        red_mat = df_pca
    if method == 'UMAP':
        red_mat= 0
    if method == "T-SNE":
        red_mat=0
    return red_mat

def load_data():
    ng20 = fetch_20newsgroups(subset='test')
    corpus = ng20.data[:2000]
    labels = ng20.target[:2000]
    df_labels = pd.DataFrame(labels)
    df_labels.rename(columns = {0:'label'}, inplace = True)
    df_corpus = pd.DataFrame(corpus)
    df_corpus.rename(columns = {0:'text'}, inplace = True)
    return df_corpus, df_labels

df_corpus, df_labels= load_data()

import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

stop_words = stopwords.words('english') # définir les stop_words
lemmatizer = WordNetLemmatizer()

def data_preprocessing(review):

  # nettoyage des données
  article = re.sub(re.compile('<.*?>'), '', review) #removing html tags
  article =  re.sub('[^A-Za-z0-9]+', ' ', review) #taking only words

  # miniscule
  article = article.lower()

  # tokenization
  tokens = nltk.word_tokenize(article) # converts articles to tokens

  # stop_words removal
  article = [word for word in tokens if word not in stop_words] #removing stop words

  # lemmatization
  article = [lemmatizer.lemmatize(word) for word in article]

  # join words in preprocessed review
  article = ' '.join(article)

  return article

# On applique le pré-traitement à nos données df_corpus
df_corpus['preprocessed_text'] = df_corpus['text'].apply(lambda article: data_preprocessing(article))

model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

embeddings = model.encode(df_corpus['preprocessed_text'])

def clust(mat,k):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(mat)
    kmeans_labels = kmeans.labels_
    return kmeans_labels

# Perform dimensionality reduction and clustering for each method
methods = ['ACP']
for method in methods:
    # Perform dimensionality reduction
    red_emb = dim_red(embeddings, 20, method,df_labels)

    # Perform clustering
    pred = clust(red_emb, 3)

    # Evaluate clustering results
    nmi_score = normalized_mutual_info_score(pred, df_labels['label'])
    ari_score = adjusted_rand_score(pred, df_labels['label'])

    # Print results
    print(f'Method: {method}\nNMI: {nmi_score:.2f} \nARI: {ari_score:.2f}\n')

